{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwCrl6mvTXnapIYUmGxS9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sendmail123/DataScience/blob/master/Pytorch/DQN/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJmhKIbjiIwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar 24 15:13:02 2020\n",
        "\n",
        "@author: vishalsingh\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import environment\n",
        "import brain\n",
        "import dqn\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = 'O'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "#SETTING THE PARAMETRES\n",
        "\n",
        "epsilon = 0.3\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions -1) / 2\n",
        "number_epochs = 100\n",
        "max_memory = 3000\n",
        "batch_size=512\n",
        "temprature_step = 1.5\n",
        "\n",
        "env = environment.Environment(optimal_temprature = (18.0,24.0), initial_month=0, initial_number_users=20, initial_rate_data=30)\n",
        "brain = brain.Brain(learning_rate=0.00001,number_actions=number_actions)\n",
        "\n",
        "dqn = dqn.DQN(max_memory=max_memory,discount=0.9)  \n",
        "\n",
        "train = True\n",
        "\n",
        "#TRAINING THE AI\n",
        "\n",
        "env.train = train\n",
        "model = brain.model\n",
        "if(env.train):\n",
        "     for epoch in range(1,number_epochs):\n",
        "         total_reward = 0\n",
        "         loss = 0.0\n",
        "         new_month=np.random.randint(0,12)\n",
        "         env.reset(new_month = new_month)\n",
        "         game_over = False\n",
        "         current_state, _ , _  = env.observe()\n",
        "         timestep = 0\n",
        "         while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
        "             \n",
        "             #Exploration\n",
        "             if (np.random.rand()) <= epsilon:\n",
        "                 action = np.random.randint(0,number_actions)\n",
        "                 if (action - direction_boundary) < 0 :\n",
        "                     direction = -1\n",
        "                 else:\n",
        "                     direction = 1\n",
        "                 energy_ai = abs(action - direction_boundary) * temprature_step\n",
        "             else:\n",
        "                q_values = model.predict(current_state)\n",
        "                action = np.argmax(q_values[0])\n",
        "                if (action - direction_boundary) < 0 :\n",
        "                     direction = -1\n",
        "                else:\n",
        "                     direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temprature_step\n",
        "                \n",
        "                next_state, reward, game_over = env.update_env(direction,energy_ai,int(timestep/(30*24*60)))\n",
        "                \n",
        "                total_reward += reward\n",
        "                \n",
        "                dqn.remember([current_state,action,reward,next_state], game_over)\n",
        "                \n",
        "                inputs, targets = dqn.get_batch(model, batch_size=batch_size)\n",
        "                \n",
        "                loss = loss + model.train_on_batch(inputs,targets)\n",
        "                timestep += 1\n",
        "                current_state = next_state\n",
        "                \n",
        "         print(\"\\n\")\n",
        "         print(\"Epochs: {:03d}/{:03d}\".format(epoch,number_epochs) )\n",
        "         print (\"Total Enerygy Spent with AI: {:.0f} \".format(env.total_energy_ai))\n",
        "         print(\"Total Enerygy Spent with no AI: {:.0f} \".format(env.total_energy_noai))\n",
        "                \n",
        "model.save(\"model.h5\")\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6MJjDHCiY3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60yelW7jiZTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}